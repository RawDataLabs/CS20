{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04.04 Eager execution (Word2Vec Visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen(download_url, expected_byte, vocab_size, batch_size, \n",
    "                skip_window, visual_fld):\n",
    "    local_dest = 'data/text8.zip'\n",
    "    utils.download_one_file(download_url, local_dest, expected_byte)\n",
    "    words = read_data(local_dest)\n",
    "    dictionary, _ = build_vocab(words, vocab_size, visual_fld)\n",
    "    index_words = convert_words_to_index(words, dictionary)\n",
    "    del words           # to save memory\n",
    "    single_gen = generate_sample(index_words, skip_window)\n",
    "    \n",
    "    while True:\n",
    "        center_batch = np.zeros(batch_size, dtype=np.int32)\n",
    "        target_batch = np.zeros([batch_size, 1])\n",
    "        for index in range(batch_size):\n",
    "            center_batch[index], target_batch[index] = next(single_gen)\n",
    "        yield center_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_words(visual_fld, num_visualize):\n",
    "    \"\"\" create a list of num_visualize most frequent words to visualize on TensorBoard.\n",
    "    saved to visualization/vocab_[num_visualize].tsv\n",
    "    \"\"\"\n",
    "    words = open(os.path.join(visual_fld, 'vocab.tsv'), 'r').readlines()[:num_visualize]\n",
    "    words = [word for word in words]\n",
    "    file = open(os.path.join(visual_fld, 'vocab_' + str(num_visualize) + '.tsv'), 'w')\n",
    "    for word in words:\n",
    "        file.write(word)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_mkdir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 파마메터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128 # 단어의 임베딩 벡터의 차원을 정한다.\n",
    "SKIP_WINDOW = 1 # 문맥 window\n",
    "NUM_SAMPLED = 64 # 숫자 부정적인 예의 샘플\n",
    "LEARNING_RATE = 1.0\n",
    "NUM_TRAIN_STEPS = 100000\n",
    "VISUAL_FLD = 'visualization'\n",
    "SKIP_STEP = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다운로드 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
    "EXPECTED_BYTES = 31344016\n",
    "NUM_VISUALIZE = 3000 # 각화 할 토큰 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel:\n",
    "    \"\"\"word2vec 모델로 부터 graph를 구성한다.\"\"\"\n",
    "    def __init__(self, dataset, vocab_size, embed_size, batch_size, num_sampled, learning_rate):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_sampled = num_sampled\n",
    "        self.lr = learning_rate\n",
    "        self.global_step = tf.get_variable('global_step', initializer=tf.constant(0), trainable=False) # 여기서 문제 발생 나중에 풀자\n",
    "        self.skip_step = SKIP_STEP\n",
    "        self.dataset = dataset\n",
    "\n",
    "    \n",
    "    def _import_data(self):\n",
    "        # Step 1: 데이터 가져오기\n",
    "        with tf.name_scope('data'):\n",
    "            self.iterator = self.dataset.make_initializable_iterator()\n",
    "            self.center_words, self.target_words = self.iterator.get_next()\n",
    "    \n",
    "    def _create_embedding(self):\n",
    "        # Step 2 + 3: 가중치 선언 그리고 embedding 색인 처리\n",
    "        with tf.name_scope('embed'):\n",
    "            self.embed_matrix = tf.get_variable('embed_matrix',\n",
    "                                               shape=[self.vocab_size, self.embed_size],\n",
    "                                               initializer=tf.random_uniform_initializer())\n",
    "            self.embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embedding')\n",
    "    \n",
    "    def _create_loss(self):\n",
    "        # Step 4: 손실 함수 정의\n",
    "        with tf.name_scope('loss'):\n",
    "            # NCE loss 변수 만든다.\n",
    "            nce_weight = tf.get_variable('nce_weight', \n",
    "                                         shape=[self.vocab_size, self.embed_size],\n",
    "                                         initializer=tf.truncated_normal_initializer(stddev=1.0 / (self.embed_size ** 0.5)))\n",
    "            nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))\n",
    "            \n",
    "            #  NCE 손실 함수를 손실 함수로 정의 한다.\n",
    "            self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight,\n",
    "                                                     biases=nce_bias,\n",
    "                                                     labels=self.target_words,\n",
    "                                                     inputs=self.embed,\n",
    "                                                     num_sampled=self.num_sampled,\n",
    "                                                     num_classes=self.vocab_size),\n",
    "                                                     name='loss')\n",
    "    \n",
    "    def _create_optimizer(self):\n",
    "        # Step 5: 옵티마이져 정의\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)\n",
    "    \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            tf.summary.histogram('histogram loss', self.loss)\n",
    "            # 한번에 합쳐 놓으면 관리하기가 편하다.\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    def build_graph(self):\n",
    "        \"\"\" 모델 graph 작성\"\"\"\n",
    "        self._import_data()\n",
    "        self._create_embedding()\n",
    "        self._create_loss()\n",
    "        self._create_optimizer()\n",
    "        self._create_summaries()\n",
    "        \n",
    "    def train(self, num_train_steps):\n",
    "        # 기본값은 모든 변수에 저장 - embed_matrix, nce_weight, nce_bias\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        initial_step = 0\n",
    "        safe_mkdir('checkpoints')\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(self.iterator.initializer)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "            \n",
    "            # 만약 체크포인트가 존재 하면 복구 시킨다.\n",
    "            if ckpt and ckpt.modle_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.modle_checkpoint_path)\n",
    "                \n",
    "            total_loss = 0.0 # 마지막 SKIP_STEP 단계의 평균 손실\n",
    "            writer = tf.summary.FileWriter('graphs/word2vec/lr' + str(self.lr), sess.graph)\n",
    "            initial_step = self.global_step.eval()\n",
    "            \n",
    "            for index in range(initial_step, initial_step + num_train_steps):\n",
    "                try:\n",
    "                    loss_batch, _, summary = sess.run([self.loss, self.optimizer, self.summary_op])\n",
    "                    writer.add_summary(summary, global_step=index)\n",
    "                    total_loss += loss_batch\n",
    "                    if (index + 1) % self.skip_step == 0:\n",
    "                        print('Average loss at step {}: {:5.1f}'.format(index, total/loss / self.skip_step))\n",
    "                        total_loss = 0.0\n",
    "                        saver.save(sess, 'checkpoints/skip-gram', index)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    sess.run(self.iterator.initializer)\n",
    "            writer.close()\n",
    "    \n",
    "    def visualize(self, visual_fid, num_visualize):\n",
    "        \"\"\" tensorboard --logdir='visualization를 통해 임베딩 된것을 볼 수 있다. \"\"\"\n",
    "        most_common_words(visual_fld, num_visualize)\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "            \n",
    "             # 만약 체크포인트가 존재 하면 복구 시킨다.\n",
    "            if ckpt and ckpt.modle_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.modle_checkpoint_path)   \n",
    "                \n",
    "            final_embed_matrix = sess.run(self.embed_matrix)\n",
    "            \n",
    "            # 새로운 변수에 가지고 있는 embedding 을 저장한다.\n",
    "            embedding_var = tf.Variable(final_embed_matrix[:num_visualize], name='embedding')\n",
    "            sess.run(embedding_var.initializer)\n",
    "            \n",
    "            cofig = projector.ProjectorConfig()\n",
    "            summary_writer = tf.summary.FileWriter(visual_fld)\n",
    "            \n",
    "            # 설정 파일에 embedding을 추가한다.\n",
    "            embedding = config.embeddings.add()\n",
    "            embedding.tensor_name = embedding_var.name\n",
    "            \n",
    "            # 사전의 단어를 텐서의 메타 데이터 파일에 링크합니다.\n",
    "            embedding.metadata_path = 'vocab_' + str(num_visualize) + '.tsv'\n",
    "            \n",
    "            # TensorBoad가 시작될때 사용할 설정 파일을 저장한다.\n",
    "            projector.visualize_embeddings(summary_writer, config)\n",
    "            saver_embed = tf.train.Saver([embedding_var])\n",
    "            saver_embed.save(sess, os.path.join(visual_fld, 'model.ckpt'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "    yield from batch_gen(DOWNLOAD_URL, EXPECTED_BYTES, VOCAB_SIZE, BATCH_SIZE, SKIP_WINDOW, VISUAL_FLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset = tf.data.Dataset.from_generator(gen, \n",
    "                                (tf.int32, tf.int32), \n",
    "                                (tf.TensorShape([BATCH_SIZE]), tf.TensorShape([BATCH_SIZE, 1])))\n",
    "    model = SkipGramModel(dataset, VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n",
    "    model.build_graph()\n",
    "    model.train(NUM_TRAIN_STEPS)\n",
    "    model.visualize(VISUAL_FLD, NUM_VISUALIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable global_step already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-16-772611d1bd85>\", line 9, in __init__\n    self.global_step = tf.get_variable('global_step', initializer=tf.constant(0), trainable=False)\n  File \"<ipython-input-17-4bc92ee3d28c>\", line 5, in main\n    model = SkipGramModel(dataset, VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n  File \"<ipython-input-18-c7bc734e5e35>\", line 2, in <module>\n    main()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-4bc92ee3d28c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                 \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 (tf.TensorShape([BATCH_SIZE]), tf.TensorShape([BATCH_SIZE, 1])))\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkipGramModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBED_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_SAMPLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_TRAIN_STEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-2525424108c2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, vocab_size, embed_size, batch_size, num_sampled, learning_rate)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_sampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_sampled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'global_step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSKIP_STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable global_step already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-16-772611d1bd85>\", line 9, in __init__\n    self.global_step = tf.get_variable('global_step', initializer=tf.constant(0), trainable=False)\n  File \"<ipython-input-17-4bc92ee3d28c>\", line 5, in main\n    model = SkipGramModel(dataset, VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n  File \"<ipython-input-18-c7bc734e5e35>\", line 2, in <module>\n    main()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

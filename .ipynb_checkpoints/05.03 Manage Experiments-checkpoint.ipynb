{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05.03 Manage Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.train.Saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래프의 변수를 바이너리 파일에 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.Saver.save(sess, save_path, global_step=None...)\n",
    "tf.train.Saver.restore(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = SkipGramModel(params)\n",
    "\n",
    "# create a saver object\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\tfor step in range(training_steps): \n",
    "\t\tsess.run([optimizer])\n",
    "\t\t\n",
    "\t\t# save model every 1000 steps\n",
    "\t\tif (step + 1) % 1000 == 0:\n",
    "\t\t\tsaver.save(sess, \n",
    "'checkpoint_directory/model_name',\n",
    "global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전역에서 증가되는 값을 옵티마이져 부분에 알릴 필요가 있다.    \n",
    "이유는 학습 속도를 떨어 뜨리는 시기를 확인 하기 위해서 이다.  \n",
    "이럴 때는 global step Variable을 사용 한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./05_images/12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장된 값 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = tf.Variable(..., name='v1') \n",
    "v2 = tf.Variable(..., name='v2') \n",
    "You can save your variables in one of three ways:\n",
    "saver = tf.train.Saver({'v1': v1, 'v2': v2})\n",
    "saver = tf.train.Saver([v1, v2])\n",
    "saver = tf.train.Saver({v.op.name: v for v in [v1, v2]}) # similar to a dict\n",
    "\n",
    "saver.restore(sess, 'checkpoints/name_of_the_checkpoint')\n",
    "#saver.restore(sess, 'checkpoints/skip-gram-99999')\n",
    "# 그래프는 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint를 검사한다.\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "\n",
    "# 유효한지 체크 한 후 가져 온다.\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "     saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "교육 과정에서 요약 통계를 시각화 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.summary.scalar  \n",
    "tf.summary.histogram  \n",
    "tf.summary.image  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소스 (모두 하나의 요약 연산으로 병합하면 더 쉽게 관리 할 수 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\", self.loss)\n",
    "    tf.summary.scalar(\"accuracy\", self.accuracy)            \n",
    "    tf.summary.histogram(\"histogram loss\", self.loss)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary 또한 Tensorflow Session이 실행 되어야 작성 됩니다. \n",
    "\n",
    "loss_batch, _, summary = sess.run([loss, optimizer, summary_op])\n",
    "\n",
    "# 글로벌 step 필요 함\n",
    "writer.add_summary(summary, global_step=step)\n",
    "\n",
    "tf.summary.scalar(\"loss\", self.loss)\n",
    "tf.summary.histogram(\"histogram loss\", self.loss)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver() # 모든 변수를 저장 한다.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    for index in range(10000):\n",
    "        ...\n",
    "        loss_batch, _, summary = sess.run([loss, optimizer, summary_op])\n",
    "        writer.add_summary(summary, global_step=index) # 글로벌 step 사용\n",
    "\n",
    "        if (index + 1) % 1000 == 0:\n",
    "            saver.save(sess, 'checkpoints/skip-gram', index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텐서 보드에서 확인 가능 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./05_images/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./05_images/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt_text](./05_images/15.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
